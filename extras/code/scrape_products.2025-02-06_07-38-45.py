# Code generated by gpt-4o-mini

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import logging

# Configure logging for better traceability
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def fetch_product_data(page_url: str) -> dict:
    """
    Fetch product data from a given URL.
    
    Args:
    - page_url (str): URL of the product page.
    
    Returns:
    - dict: A dictionary with product data or an empty dictionary if no data could be fetched.
    """
    try:
        response = requests.get(page_url, timeout=10)
        response.raise_for_status()  # Raise an error for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')
        
        product_name = soup.find('h1', class_='product-title').get_text(strip=True)
        price = soup.find('span', class_='price').get_text(strip=True)
        description = soup.find('div', class_='product-description').get_text(strip=True)
        
        rating_tag = soup.find('div', class_='rating')
        rating = rating_tag.get_text(strip=True) if rating_tag else 'No rating'
        
        return {
            "product_name": product_name,
            "price": price,
            "description": description,
            "rating": rating
        }

    except requests.RequestException as e:
        logging.error(f"Network error while fetching {page_url}: {e}")
    except AttributeError as e:
        logging.error(f"Missing data in the HTML structure for {page_url}: {e}")
    return {}

def scrape_products(urls: list) -> pd.DataFrame:
    """
    Scrape product information from a list of URLs and store it in a DataFrame.
    
    Args:
    - urls (list): List of product page URLs.
    
    Returns:
    - pd.DataFrame: DataFrame containing the scraped product data.
    """
    product_data_list = []
    
    for url in urls:
        logging.info(f"Fetching data from {url}")
        product_data = fetch_product_data(url)
        
        if product_data:  # Only append if data is found
            product_data_list.append(product_data)
        
        # Randomized delay to respect the server load
        time.sleep(random.uniform(1.5, 3.0))
    
    # Return the DataFrame, including handling the edge case of empty lists
    return pd.DataFrame(product_data_list) if product_data_list else pd.DataFrame(columns=["product_name", "price", "description", "rating"])

# Example usage:
if __name__ == "__main__":
    product_urls = [
        "https://example.com/product/page/1",
        "https://example.com/product/page/2",
        # Add more URLs as needed
    ]

    dataframe = scrape_products(product_urls)
    print(dataframe)

# DISCLAIMER: Ensure compliance with the website's terms of service and robots.txt.
# Always obtain permission if necessary before scraping. Use responsibly.