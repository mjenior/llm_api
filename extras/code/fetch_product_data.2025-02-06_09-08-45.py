# Code generated by gpt-4o

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib.robotparser

# Sample URLs to scrape from
URLs = [
    "https://example-ecommerce.com/product1",
    "https://example-ecommerce.com/product2",
]

# Function to read robots.txt and check disallowed paths
def can_scrape(base_url):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(base_url + "/robots.txt")
    rp.read()
    return rp.can_fetch("*", base_url)

# Function to fetch and parse product details from a URL
def fetch_product_data(url):
    try:
        # Request headers to simulate a browser visit
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        # Perform request and parse content
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Check for HTTP errors
        
        # Parse HTML content using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract product details
        product_name = soup.find("h1", class_="product-name").text.strip()
        product_price = soup.find("span", class_="product-price").text.strip()
        product_description = soup.find("div", class_="product-description").text.strip()
        # Use get_attribute_list() to safely extract possibly missing rating
        product_rating = soup.find("span", class_="product-rating").get_text(strip=True) if soup.find("span", class_="product-rating") else "No rating"
        
        return {
            "Name": product_name,
            "Price": product_price,
            "Description": product_description,
            "Rating": product_rating
        }
    except (requests.RequestException, ValueError) as e:
        print(f"Error fetching {url}: {e}")
        return None

# Check robots.txt compliance and scrape data
base_url = "https://example-ecommerce.com"
if can_scrape(base_url):
    products_data = []
    for url in URLs:
        product_data = fetch_product_data(url)
        if product_data:
            products_data.append(product_data)
        
        # Respectful pause to prevent server overload
        time.sleep(2)
    
    # Create DataFrame from the extracted product data
    df = pd.DataFrame(products_data)
    print(df)
else:
    print("Scraping is not allowed according to robots.txt")