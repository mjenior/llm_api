# Code generated by gpt-4o-mini

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib.robotparser
from typing import List, Dict, Optional

# Sample URLs to scrape from
URLs: List[str] = [
    "https://example-ecommerce.com/product1",
    "https://example-ecommerce.com/product2",
]

def can_scrape(base_url: str) -> bool:
    """
    Check the robots.txt file to determine if scraping is allowed.
    
    Args:
        base_url (str): The base URL of the site to be scraped.
        
    Returns:
        bool: True if scraping is allowed, False otherwise.
    """
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(base_url + "/robots.txt")
    rp.read()
    return rp.can_fetch("*", base_url)

def fetch_product_data(url: str) -> Optional[Dict[str, str]]:
    """
    Fetch product details from a given URL.
    
    Args:
        url (str): The product URL to scrape.
        
    Returns:
        Optional[Dict[str, str]]: A dictionary containing product details or None if an error occurs.
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract product details using BeautifulSoup
        product_name = soup.find("h1", class_="product-name").get_text(strip=True)
        product_price = soup.find("span", class_="product-price").get_text(strip=True)
        product_description = soup.find("div", class_="product-description").get_text(strip=True)
        product_rating = soup.find("span", class_="product-rating").get_text(strip=True) \
            if soup.find("span", class_="product-rating") else "No rating"
        
        return {
            "Name": product_name,
            "Price": product_price,
            "Description": product_description,
            "Rating": product_rating
        }
    except requests.RequestException as e:
        print(f"Network error while fetching {url}: {e}")
        return None
    except ValueError as e:
        print(f"Value error while processing {url}: {e}")
        return None

def scrape_products(urls: List[str], delay: int = 2) -> pd.DataFrame:
    """
    Scrape product data from a list of URLs and return a DataFrame.
    
    Args:
        urls (List[str]): List of product URLs to scrape.
        delay (int): Time delay (in seconds) to wait between requests to prevent overloading the server.
        
    Returns:
        pd.DataFrame: A DataFrame containing the scraped product data.
    """
    products_data = []
    base_url = "https://example-ecommerce.com"

    if can_scrape(base_url):
        for url in urls:
            product_data = fetch_product_data(url)
            if product_data:
                products_data.append(product_data)
            time.sleep(delay)  # Respectful pause between requests

    else:
        print("Scraping is not allowed according to robots.txt")

    return pd.DataFrame(products_data)

# Execute scraping and print results
df = scrape_products(URLs)
print(df)