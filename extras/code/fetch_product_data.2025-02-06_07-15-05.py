# Code generated by gpt-4o-mini

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from typing import List, Dict, Any

def fetch_product_data(url: str) -> Dict[str, Any]:
    """Fetch product data from the specified URL and return a dictionary of details."""
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')

        # Example parsing logic (actual selectors may vary)
        name = soup.select_one('h1.product-title').text.strip()
        price = soup.select_one('span.product-price').text.strip()
        description = soup.select_one('div.product-description').text.strip()
        rating = soup.select_one('span.product-rating').text.strip() if soup.select_one('span.product-rating') else None

        return {
            'name': name,
            'price': price,
            'description': description,
            'rating': rating
        }
    except requests.RequestException as e:
        print(f"Network error: {e}")
        return {}
    except Exception as e:
        print(f"An error occurred: {e}")
        return {}

def scrape_products(urls: List[str]) -> pd.DataFrame:
    """Scrape product data from a list of URLs and return a DataFrame."""
    products = []
    for url in urls:
        data = fetch_product_data(url)
        if data and data['name']:  # Ensure we have a valid entry
            products.append(data)
        time.sleep(1)  # Respectful delay
    return pd.DataFrame(products)

# Example usage
urls_to_scrape = [
    "https://example.com/product1",
    "https://example.com/product2"
]
df = scrape_products(urls_to_scrape)
print(df)import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from typing import List, Dict, Optional, Any

def fetch_product_data(url: str) -> Optional[Dict[str, Any]]:
    """
    Fetch product data from the specified URL and return a dictionary of details.
    
    Args:
        url (str): The URL of the product page to scrape.

    Returns:
        Optional[Dict[str, Any]]: A dictionary containing product details if successful; 
                                   None if an error occurs.
    """
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raises an error for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extracting product details using CSS selectors
        name = soup.select_one('h1.product-title').text.strip()
        price = soup.select_one('span.product-price').text.strip()
        description = soup.select_one('div.product-description').text.strip()
        rating = soup.select_one('span.product-rating')
        rating_text = rating.text.strip() if rating else None  # Handle missing ratings

        return {
            'name': name,
            'price': price,
            'description': description,
            'rating': rating_text
        }
    except requests.RequestException as e:
        print(f"Network error while fetching {url}: {e}")
    except Exception as e:
        print(f"An error occurred while parsing {url}: {e}")

    return None

def scrape_products(urls: List[str]) -> pd.DataFrame:
    """
    Scrape product data from a list of URLs and return a pandas DataFrame.
    
    Args:
        urls (List[str]): List of product URLs to scrape.

    Returns:
        pd.DataFrame: DataFrame containing scraped product data.
    """
    products = []
    for url in urls:
        data = fetch_product_data(url)
        if data:  # Add data if it's not None
            products.append(data)
        time.sleep(1)  # Respect politeness policy with a delay

    # Returning DataFrame for easier analysis and processing
    return pd.DataFrame(products)

# Example usage
urls_to_scrape = [
    "https://example.com/product1",
    "https://example.com/product2"
]

# Execute the scraping process and print the results
df = scrape_products(urls_to_scrape)
print(df)