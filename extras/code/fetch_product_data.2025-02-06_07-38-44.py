# Code generated by gpt-4o

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import urlparse, urljoin
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def fetch_product_data(page_url):
    """
    Fetch product data from a given URL.
    
    Args:
    page_url (str): URL of the product page.
    
    Returns:
    dict: A dictionary with product data or None if no data could be fetched.
    """
    try:
        # Request the webpage
        response = requests.get(page_url, timeout=10)
        response.raise_for_status()  # Raise an error for bad responses
        
        # Parse the webpage content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract product data assuming structured HTML
        product_name = soup.find('h1', class_='product-title').get_text(strip=True)
        price = soup.find('span', class_='price').get_text(strip=True)
        description = soup.find('div', class_='product-description').get_text(strip=True)
        # Use get_text with default value to handle missing ratings
        rating_tag = soup.find('div', class_='rating')
        rating = rating_tag.get_text(strip=True) if rating_tag else 'No rating'
        
        return {
            "product_name": product_name,
            "price": price,
            "description": description,
            "rating": rating
        }

    except requests.RequestException as e:
        logging.error(f"Network error while fetching {page_url}: {e}")
    except AttributeError as e:
        logging.error(f"Missing data in the HTML structure for {page_url}: {e}")
    return None

def scrape_products(urls):
    """
    Scrape product information from a list of URLs and store it in a DataFrame.
    
    Args:
    urls (list): List of product page URLs.
    
    Returns:
    pd.DataFrame: DataFrame containing the scraped product data.
    """
    product_data_list = []
    for url in urls:
        logging.info(f"Fetching data from {url}")
        product_data = fetch_product_data(url)
        if product_data:
            product_data_list.append(product_data)
        
        # Respectful scraping: Wait to reduce server load
        time.sleep(2)
    
    # Convert list of dictionaries to a pandas DataFrame
    df = pd.DataFrame(product_data_list)
    return df

# Example of usage:
product_urls = [
    "https://example.com/product/page/1",
    "https://example.com/product/page/2",
    # Add URLs here
]

dataframe = scrape_products(product_urls)
print(dataframe)

# DISCLAIMER: Ensure compliance with the website's terms of service and robots.txt.
# Always obtain permission if necessary before scraping. Use responsibly.