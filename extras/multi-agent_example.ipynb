{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d824fc2",
   "metadata": {},
   "source": [
    "# Example Worflow Using Multiple LLM Agents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf7409c7-3c2b-4ec5-aff2-f869d848f80d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This notebook is an example of how you can use the <promptpal> package to quickly create specialized LLM agents to complete tasks alone or in cooperation with other agents you create. Each agent initialized below makes use of several of the built-in options in different ways tailored to the specific task they are meant for. By default for all agents, all text processing and response text are reported to StdOut (verbose=True) and saved to a log file (logging=True). Any generated code snippets are also saved to executable scripts in a new code folder in your working directory (save_code=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7907ea1d-e1d9-4b83-8fb2-6fd503095d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core class\n",
    "from promptpal.core import CreateAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ffcc6",
   "metadata": {},
   "source": [
    "## Initialize distinct agents with unique expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ec5244-6889-4854-a27e-bed1e5e6e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o\n",
      "    Role: Full Stack Developer\n",
      "    \n",
      "    Chain-of-thought: True\n",
      "    Prompt refinement: True\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 1\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: True\n",
      "    Time stamp: 2025-02-06_09-17-48\n",
      "    Assistant ID: asst_gU8OVhTXKDkml0s6BLNKHqbi\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Sr. App Developer, with Chain of Thought Tracking and automated prompt refinement\n",
    "dev = CreateAgent(role=\"developer\", model=\"gpt-4o\", refine=True, chain_of_thought=True, save_code=True)\n",
    "# The more complex tasks are given to a larger model than the default gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc97852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o-mini\n",
      "    Role: Refactoring Expert\n",
      "    \n",
      "    Chain-of-thought: False\n",
      "    Prompt refinement: False\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 1\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: True\n",
      "    Time stamp: 2025-02-06_09-17-49\n",
      "    Assistant ID: asst_hUjh9Ums7LZXrdUO5rvjTiW3\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Code refactoring and formatting expert\n",
    "recode = CreateAgent(role=\"refactor\", save_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db99dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o-mini\n",
      "    Role: Unit Tester\n",
      "    \n",
      "    Chain-of-thought: False\n",
      "    Prompt refinement: False\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 1\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: True\n",
      "    Time stamp: 2025-02-06_09-17-49\n",
      "    Assistant ID: asst_Mk4nQFX7N39Iz9LfdXRjG24r\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Unit test generator\n",
    "test = CreateAgent(role=\"tester\", save_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8a0fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o\n",
      "    Role: Writer\n",
      "    \n",
      "    Chain-of-thought: False\n",
      "    Prompt refinement: False\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 3\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: False\n",
      "    Time stamp: 2025-02-06_09-17-50\n",
      "    Assistant ID: asst_vxPrAwSqsUdruYESrnMIG2IG\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Creative science and technology writer, with Chain of Thought Tracking, and multi-reponse concencus\n",
    "write = CreateAgent(role=\"writer\", model=\"gpt-4o\", iterations=3, chain_of_thought=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o-mini\n",
      "    Role: Editor\n",
      "    \n",
      "    Chain-of-thought: False\n",
      "    Prompt refinement: False\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 1\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: False\n",
      "    Time stamp: 2025-02-06_09-17-50\n",
      "    Assistant ID: asst_aMalGp76z0OPlXmLp9YJSHGP\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Expert copy editor\n",
    "edit = CreateAgent(role=\"editor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c6b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent parameters:\n",
      "    Model: gpt-4o-mini\n",
      "    Role: User-defined custom role\n",
      "    \n",
      "    Chain-of-thought: False\n",
      "    Prompt refinement: False\n",
      "    Associative glyphs: False\n",
      "    Response iterations: 1\n",
      "    Subdirectory scanning: False\n",
      "    Text logging: True\n",
      "    Verbose StdOut: True\n",
      "    Code snippet detection: False\n",
      "    Time stamp: 2025-02-06_09-17-51\n",
      "    Assistant ID: asst_mw5tGC2PuZNU2bWl8CDxopsN\n",
      "    Thread ID: thread_DXlENUO7whoNrAuPFyCjzZSv\n",
      "    Seed: 111010000110110001\n",
      "    Requests in current thread: 0\n",
      "    Current total cost: $0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Custom role for condensing text into Slack posts\n",
    "role_text = \"\"\"\n",
    "You are an expert in condensing text and providing summaries.\n",
    "Begin by providing a brief summary of the text. Then, condense the text into a few key points. \n",
    "Finally, write a concise conclusion that captures the main ideas of the text.\n",
    "Remember to keep the summary clear, concise, and engaging.\n",
    "Use emojis where appropriate\n",
    "Keep posts to a approximately 1 paragraph of 3-4 sentences.\n",
    "Add a @here mention at the beginning of the message to notify the channel members about the summary.\n",
    "\"\"\"\n",
    "slack = CreateAgent(role=role_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07c3e3",
   "metadata": {},
   "source": [
    "## Submit requests to the new agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba4e1f7-4f0c-4d55-8942-8b7d113b3918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt-4o-mini optimizing initial user request...\n",
      "\n",
      "Refined query prompt:\n",
      "Write a Python script that effectively scrapes data from a collection of webpages and reformats the extracted information into a structured dataframe for subsequent analysis. The target webpages focus on product listings from an e-commerce platform, where key data points such as product name, price, description, and rating will be gathered. You should ensure that the webpages are accessible and do not require any form of authentication for access.\n",
      "\n",
      "To accomplish this task, you should utilize the `requests` library to obtain the content of the webpages and the `BeautifulSoup` module from `bs4` for parsing the HTML structure effectively. Itâ€™s crucial to implement robust handling for potential issues such as missing data fields, like a product lacking a rating, to ensure the script runs smoothly without failures. The scraped data should be stored in a pandas dataframe with clearly defined column names for easy reference.\n",
      "\n",
      "Furthermore, you should include basic error handling mechanisms to manage situations such as network connectivity issues or invalid URLs. It is important to respect the website's `robots.txt` file and to incorporate a reasonable delay between requests to prevent overwhelming the server. Ensure that no personally identifiable information (PII) or sensitive data is collected during the scraping process. Lastly, your code should be well-commented to clarify each key step, providing a clear understanding of its functionality.\n",
      "\n",
      "gpt-4o processing updated conversation thread...\n",
      "\n",
      "<thinking>\n",
      "\n",
      "To accomplish the task of scraping an e-commerce platform's product listings, I will follow these steps:\n",
      "\n",
      "1. **Setup Environment**: Ensure the necessary Python libraries (`requests`, `bs4`, `pandas`, and possibly `time`) are installed and import them into the script.\n",
      "\n",
      "2. **Define URL List**: Create a list of URLs to be scraped. We assume these URLs are public and do not require authentication.\n",
      "\n",
      "3. **Robust HTTP Request Handling**: Use the `requests` library to retrieve webpage content. Implement checks for response status codes to handle network errors and invalid URLs.\n",
      "\n",
      "4. **HTML Parsing**: Utilize `BeautifulSoup` from `bs4` to parse page content. Identify HTML tags and attributes that encapsulate the product's name, price, description, and rating.\n",
      "\n",
      "5. **Data Extraction and Handling Missing Data**: Extract data for each product while gracefully handling missing or malformed data, such as missing ratings. Store this in structured form.\n",
      "\n",
      "6. **Data Storage in DataFrame**: Use `pandas` to structure the extracted data into a DataFrame with columns: `product_name`, `price`, `description`, and `rating`.\n",
      "\n",
      "7. **Respect Server Policies**: Implement a delay, as guided by `robots.txt`, to avoid overwhelming the target server.\n",
      "\n",
      "8. **Commenting and Documentation**: Include detailed comments explaining each step and error handling code.\n",
      "\n",
      "</thinking>\n",
      "\n",
      "<reflection>\n",
      "\n",
      "While this setup should cover the key requirements, I must ensure clarity in parsing HTML elements to capture product details accurately across different pages. If the HTML structure varies significantly, the script may require adaptation to handle different patterns or use regular expressions.\n",
      "\n",
      "</reflection>\n",
      "\n",
      "<output>\n",
      "\n",
      "Here is the complete Python script for web scraping the e-commerce product listings:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import time\n",
      "\n",
      "# List of URLs to scrape\n",
      "urls = [\n",
      "    'https://example-ecommerce.com/page1',\n",
      "    'https://example-ecommerce.com/page2',\n",
      "    # Add more URLs as needed\n",
      "]\n",
      "\n",
      "# A function that takes a URL and returns the parsed HTML content\n",
      "def get_page_content(url):\n",
      "    try:\n",
      "        response = requests.get(url)\n",
      "        response.raise_for_status()  # May raise an HTTPError\n",
      "        return BeautifulSoup(response.content, 'html.parser')\n",
      "    except requests.exceptions.RequestException as e:\n",
      "        print(f\"Error fetching {url}: {e}\")\n",
      "        return None\n",
      "\n",
      "# A list to store the extracted data\n",
      "product_data = []\n",
      "\n",
      "for url in urls:\n",
      "    print(f\"Scraping {url}...\")\n",
      "    soup = get_page_content(url)\n",
      "    \n",
      "    if soup is None:\n",
      "        continue  # Skip this URL in case of request failure\n",
      "    \n",
      "    # Assuming products are within a specific section (update selectors as per actual HTML structure)\n",
      "    products = soup.find_all('div', class_='product-listing')\n",
      "    \n",
      "    for product in products:\n",
      "        try:\n",
      "            name = product.find('h2', class_='product-name').get_text(strip=True)\n",
      "        except AttributeError:\n",
      "            name = None\n",
      "        \n",
      "        try:\n",
      "            price = product.find('span', class_='product-price').get_text(strip=True)\n",
      "        except AttributeError:\n",
      "            price = None\n",
      "\n",
      "        try:\n",
      "            description = product.find('p', class_='product-description').get_text(strip=True)\n",
      "        except AttributeError:\n",
      "            description = None\n",
      "\n",
      "        try:\n",
      "            rating = product.find('span', class_='product-rating').get_text(strip=True)\n",
      "        except AttributeError:\n",
      "            rating = None\n",
      "        \n",
      "        # Append the data to the list\n",
      "        product_data.append({\n",
      "            'product_name': name,\n",
      "            'price': price,\n",
      "            'description': description,\n",
      "            'rating': rating\n",
      "        })\n",
      "    \n",
      "    # Respect the website's server load\n",
      "    time.sleep(1)\n",
      "\n",
      "# Convert list to DataFrame\n",
      "df = pd.DataFrame(product_data)\n",
      "\n",
      "# Display the first few rows of the DataFrame\n",
      "print(df.head())\n",
      "\n",
      "# You may choose to save the results to a file\n",
      "# df.to_csv('products.csv', index=False)\n",
      "```\n",
      "\n",
      "### Explanation of the Script:\n",
      "- **requests.get(url)**: Retrieves HTML page content.\n",
      "- **raise_for_status()**: Handles HTTP errors such as 404 Not Found.\n",
      "- **BeautifulSoup**: Parses HTML to extract data based on known tags.\n",
      "- **Pandas DataFrame**: Structures extracted data for analysis.\n",
      "- **Error handling**: Uses `try-except` blocks for attribute fetching.\n",
      "- **Respect robots.txt**: Delay implemented via `time.sleep`.\n",
      "\n",
      "Adapt the HTML selector logic (`find`/`find_all`) based on the actual structure of the target webpage. \n",
      "\n",
      "</output>\n",
      "\n",
      "Extracted code saved to:\n",
      "\tget_page_content.2025-02-06_09-17-48.py\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make initial request to first agent for computational biology project\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mWrite a Python script to scrape data from a set of webpages and reformat it into a structured dataframe for downstream analysis. \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mThe target webpages are product listings on an e-commerce website, and the data to be extracted includes: product name, price, description, and rating. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Do not hard-code any URLs or sensitive information into the script.\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mdev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/promptpal/core.py:367\u001b[0m, in \u001b[0;36mCreateAgent.request\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_image_request()\n\u001b[0;32m--> 367\u001b[0m token_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gen_token_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_print(token_report, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Check current scope thread\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/promptpal/core.py:499\u001b[0m, in \u001b[0;36mCreateAgent._gen_token_report\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     prompt_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_cost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], prompt_rate)\n\u001b[1;32m    496\u001b[0m     completion_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_cost(\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m], completion_rate\n\u001b[1;32m    498\u001b[0m     )\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_cost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_cost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompletion_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCurrent total tokens generated by this agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  ($\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - prompt tokens (i.e. input): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  ($\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - completion tokens (i.e. output): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  ($\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# Make initial request to first agent for computational biology project\n",
    "query = \"\"\"\n",
    "Write a Python script to scrape data from a set of webpages and reformat it into a structured dataframe for downstream analysis. \n",
    "The target webpages are product listings on an e-commerce website, and the data to be extracted includes: product name, price, description, and rating. \n",
    "Assume the webpages are accessible and do not require authentication.\n",
    "\n",
    "Requirements:\n",
    "    Use the requests library to fetch the webpage content and BeautifulSoup from bs4 for parsing HTML.\n",
    "    Handle potential issues such as missing data fields (e.g., if a product does not have a rating) gracefully.\n",
    "    Store the scraped data in a pandas dataframe with appropriate column names.\n",
    "    Include basic error handling (e.g., for network issues or invalid URLs).\n",
    "    Ensure the script respects the website's robots.txt file and includes a reasonable delay between requests to avoid overloading the server.\n",
    "    Do not scrape any personally identifiable information (PII) or sensitive data.\n",
    "    Include comments in the code to explain key steps.\n",
    "\n",
    "Example Input:\n",
    "    A list of URLs for product pages on an e-commerce site.\n",
    "\n",
    "Example Output:\n",
    "    A pandas dataframe with columns: product_name, price, description, and rating.\n",
    "\n",
    "Guardrails:\n",
    "    Do not scrape data at a frequency that could be considered abusive or violate the website's terms of service.\n",
    "    Include a disclaimer in the script comments reminding users to check the legality of scraping the target website and to obtain permission if necessary.\n",
    "    Do not hard-code any URLs or sensitive information into the script.\n",
    "\"\"\"\n",
    "dev.request(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e145f-a065-4ec2-acbf-268524275d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize and document any new code\n",
    "recode.request(\"Refactor the previously generated code for optimal efficiency, useability, and documentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b6b45-6fcf-4f56-a37f-85494fb58134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unit tests\n",
    "test.request(\"Generate unit test for the newly optimized code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d690104-d6e9-44bb-b48e-922122d8aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize the writer agent to generate an informed post on the background and utility of the newly created pipeline\n",
    "query = \"\"\"\n",
    "Write a biotechnology blog post about the content of the conversation and refactored code.\n",
    "Include relevant background that would necessitate this type of analysis, and add at least one example use case for the workflow.\n",
    "Extrapolate how the pipeline may be useful in cell engineering efforts, and what future improvements could lead to with continued work.\n",
    "The resulting post should be at least 3 paragraphs long with 4-5 sentences in each.\n",
    "Speak in a conversational tone and cite all sources with biological relevance to you discussion.\n",
    "\"\"\"\n",
    "write.request(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457bd15-07e1-4d20-b766-4bbb7f45cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the rough draft text to the editor agent to recieve a more finalize version\n",
    "edit.request(\"Rewrite the previous blog post for maximum readability for a general audience.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Slack post summarizing the finalized text\n",
    "slack.request(\"Create a Slack announcement for the finalized blog post.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ef4a4-3848-4605-8eca-61609911bb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
